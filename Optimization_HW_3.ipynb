{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a6fcbb6",
   "metadata": {},
   "source": [
    "# Домашнее задание 3\n",
    "\n",
    "Это домашнее задание по материалам второго семинаров. Дедлайн по отправке - 23:55 24 февраля. \n",
    "\n",
    "Домашнее задание выполняется в этом же Jupyter Notebook'e и присылается мне на почту: __beznosikov.an@phystech.edu__.\n",
    "\n",
    "Решение каждой задачи необходимо поместить после её условия.\n",
    "\n",
    "Файл должен называться: Фамилия_Имя_Optimization_HW_3\n",
    "\n",
    "При полном запуске Вашего решения (Kernel -> Restart & Run All) все ячейки должны выполняться без ошибок. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308d49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time \n",
    "np.random.seed(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a8d32",
   "metadata": {},
   "source": [
    "## Задача 1\n",
    "\n",
    "Вновь рассмотрим задачу минимизации эмпирического риска:\n",
    "\\begin{equation}\n",
    "\\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum\\limits_{i=1}^n l (g(w, x_i), y_i).\n",
    "\\end{equation}\n",
    "\n",
    "В прошлом задании работа шла с линейной модель $g(w, x) = w^T x$ и квадратичную функцию потерь $l(z, y) = (z-y)^2$. \n",
    "\n",
    "__(а)__ В дополнение к квадратичной функции потерь реализуйте логистическую/сигмоидную: $l(z,y) = \\ln (1 + \\exp(-yz))$ (__Важно: $y$ должен принимать значения $-1$ или $1$__). Выпишите градиент. Является ли новая задача регресии выпуклой? Оцените $L$ для новой функции потерь. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ed1d8",
   "metadata": {},
   "source": [
    "Имеем следующую функцию эмпирического риска: $$f(w) = \\dfrac{1}{n} \\sum\\limits_{i=1}^n \\ln\\left(1 + \\exp (-y_i w^Tx^i)\\right)$$\n",
    "Тогда ее градиент: $$\\nabla f(w) = \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\dfrac{-y_i \\exp(-y_i w^T x^i)}{1 + \\exp(-y_i w^T x^i)} \\cdot x^i$$\n",
    "А гессиан: $$ \\nabla^2 f(w) = \\dfrac{1}{n} \\sum\\limits_{i=1}^n \\dfrac{\\exp(-y_i w^Tx^i)}{(1 + \\exp(-y_i w^Tx^i))^2} \\cdot x^i(x^i)^T$$\n",
    "Имеем: $\\forall x^i \\in \\mathbb{R}^d \\hookrightarrow x^i(x^i)^T \\succeq 0$ и экспонента неотрицательна $\\Rightarrow$ задача является выпуклой.\n",
    "\n",
    "Константу липшица оценим следующим образом: $\\forall x \\hookrightarrow \\dfrac{e^x}{(1 + e^x)^2} \\leq \\dfrac{1}{4}$, поэтому: $$\\nabla^2 f(w) \\preccurlyeq \\dfrac{1}{4n} \\sum\\limits_{i=1}^n x^i(x^i)^T $$\n",
    "Тогда константа L: \n",
    "$$L = \\dfrac{1}{4n} \\cdot \\lambda_{max}(\\sum\\limits_{i=1}^n x^i(x^i)^T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb57fe",
   "metadata": {},
   "source": [
    "__(б)__ Возьмите датасет _mushrooms_ из прошлого задания. Проделайте следующие шаги из прошлого задания, только с логистической функцией потерь:\n",
    "\n",
    "1) Разделите данные на две части: обучающую и тестовую.\n",
    "\n",
    "2) Для обучающей части $X_{train}$, $y_{train}$ оцените константу $L$ задачи обучения/оптимизации.\n",
    "\n",
    "3) Используя градиентный спуск, обучите новую модель (без ограничений и регуляризаций). Постройте график: точность от номера итерации.\n",
    "\n",
    "4) Если в пункте 3) пришлось столкнуться с проблемами или просто необходимо улучшить точность, то добавьте ограничения или $\\ell_2$-регуляризацию, как в прошлом ДЗ.\n",
    "\n",
    "5) Сравните с результатами квадратичной функции потерь из прошлого ДЗ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45416fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подгружаем данные\n",
    "dataset = \"mushrooms.txt\" \n",
    "data = load_svmlight_file(dataset)\n",
    "X, y = data[0].toarray(), data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41466f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Меняем двойки на нули\n",
    "y = pd.DataFrame(y)\n",
    "y = np.array(y.replace(2, 0)).reshape(8124,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b1db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Класс логрегрессии\n",
    "def sigmoid(h):\n",
    "    return 1. / (1 + np.exp(-h))\n",
    "\n",
    "class MyLogisticRegression(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "    \n",
    "    def fit(self, X, y, lr, lambd = False, eps = 10**(-3)):\n",
    "        \n",
    "        n, k = X.shape\n",
    "        \n",
    "        if self.w is None:\n",
    "            self.w = np.random.randn(k)\n",
    "        \n",
    "        grad = np.random.randn(k)\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        self.accuracy = []\n",
    "        self.accuracy.append(self.w)\n",
    "        \n",
    "        while np.linalg.norm(grad, ord = 2) > eps:\n",
    "            \n",
    "            z = sigmoid(X @ self.w)\n",
    "            \n",
    "            grad = self._calc_grad(X, y, z, lambd)\n",
    "            \n",
    "            if len(losses) % 150 == 0:\n",
    "                self.accuracy.append(self.w - lr * grad)\n",
    "            \n",
    "            self.w -= lr * grad\n",
    "            \n",
    "            losses.append(self._loss(y, z))\n",
    "\n",
    "        return losses\n",
    "        \n",
    "    def _calc_grad(self, X, y, z, lambd):\n",
    "        \n",
    "        if bool(lambd) == True :\n",
    "            lambdaI = 2 * lambd * np.eye(self.w.shape[0])\n",
    "            grad = np.dot(X.T, (z - y)) / len(y) + lambdaI @ self.w\n",
    "            \n",
    "        grad = np.dot(X.T, (z - y)) / len(y)  \n",
    "        \n",
    "        return grad \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return sigmoid(X @ self.w)\n",
    "\n",
    "    def predict(self, X, threshold = 0.5):\n",
    "        return self.predict_proba(X) >= threshold\n",
    "    \n",
    "    def get_accuracy(self, X, y):\n",
    "        accuracy = []\n",
    "        for i in range(len(self.accuracy)):\n",
    "            accuracy.append(accuracy_score(np.round(sigmoid(X @ self.accuracy[i])), y))\n",
    "        \n",
    "        return accuracy\n",
    "      \n",
    "    def _loss(self, y, p):\n",
    "        p = np.clip(p, 1e-10, 1 - 1e-10)\n",
    "        return np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0bcab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Делим данные\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81306b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Константа липшица данной задачи:  5.25\n"
     ]
    }
   ],
   "source": [
    "#Оцениваем L:\n",
    "summ = 0\n",
    "for i in range(len(X_train)):\n",
    "    summ += X_train[i][np.newaxis]  @ X_train[i][np.newaxis].T\n",
    "L = 1 / (4 * X_train.shape[0]) * np.max(np.linalg.eigvals(summ))\n",
    "print('Константа липшица данной задачи: ', L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc04364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность по метрике accuracy_score при epsilon = 0.01:  0.9901538461538462\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Смотрим точность по метрике accuracy_score. Это логично, тк мы решаем задачу классификации.\n",
    "regressor = MyLogisticRegression()\n",
    "regressor.fit(X_train, y_train, 1/L, eps = 10**(-2))\n",
    "accuracy1 = regressor.get_accuracy(X_test, y_test)\n",
    "print(r'Точность по метрике accuracy_score при epsilon = 0.01: ', accuracy_score(y_test, regressor.predict(X_test)))\n",
    "regressor.fit(X_train, y_train, 1/L, eps = 10**(-3))\n",
    "accuracy2 = regressor.get_accuracy(X_test, y_test)\n",
    "print(r'Точность по метрике accuracy_score для epsilon = 0.001: ', accuracy_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c00e10",
   "metadata": {},
   "source": [
    "Как видно, точность получилась хорошая, скорость сходимости тоже. Проблем с пунктом 3 не возникло."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Чтобы сравнить в пункте д с предыдущим заданием, тк там я пользовался r2_score\n",
    "regressor = MyLogisticRegression()\n",
    "regressor.fit(X_train, y_train, 1/L, eps = 10**(-2))\n",
    "print(r'Точность по метрике r2_score при epsilon = 0.01: ', r2_score(y_test, regressor.predict(X_test)))\n",
    "regressor.fit(X_train, y_train, 1/L, eps = 10**(-3))\n",
    "print(r'Точность по метрике r2_score при epsilon = 0.001: ', r2_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize = (15, 6))\n",
    "x1 = 150 * np.array(range(len(accuracy1)))\n",
    "x2 = 150 * np.array(range(len(accuracy2)))\n",
    "axs[0].plot(x1[1:], accuracy1[1:], label = r'$||\\;\\nabla f\\;||_2 < 10^{-2} $')\n",
    "axs[0].set_title(\"Зависимость точности предсказаний от номера итерации\")\n",
    "axs[0].set_ylabel(\"Точность по метрике accuracy_score\")\n",
    "axs[0].set_xlabel('Номер итерации')\n",
    "axs[0].grid(alpha = 0.2)\n",
    "axs[0].legend(loc = 'center right')\n",
    "\n",
    "axs[1].plot(x2[1:], accuracy2[1:], label = r'$||\\;\\nabla f\\;||_2 < 10^{-3} $')\n",
    "axs[1].set_title(\"Зависимость точности предсказаний от номера итерации\")\n",
    "axs[1].set_ylabel(\"Точность по метрике accuracy_score\")\n",
    "axs[1].set_xlabel('Номер итерации')\n",
    "axs[1].grid(alpha = 0.2)\n",
    "axs[1].legend(loc = 'center right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83634fd0",
   "metadata": {},
   "source": [
    "Первое значимое отличие от обычной линейной регрессии в количестве итераций: при использованиии данной модели до точности $0.01$ получается примерно одинаковое число итераций и чуть хуже r2_score ($\\approx 0.99$ против $\\approx 0.97$). При одинаковом же значении эпсилон в критерии получается около $18000$ итераций, но и r2_score = $1.0$ уже на $11000$, чего не получалось добиться в предыдущем дз."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d96a89",
   "metadata": {},
   "source": [
    "## Задача 2\n",
    "\n",
    "__(a)__ Реализуйте метод тяжелого шарика. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd30eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHeavyBall(MyLogisticRegression):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs) # передаем именные параметры родительскому конструктору\n",
    "        self.w = None\n",
    "    \n",
    "    def fit(self, X, y, lr, beta, lambd = False, eps = 10**(-2)):\n",
    "        \n",
    "        if self.w is None: self.w = np.random.randn(2, X.shape[1])\n",
    "        \n",
    "        grad = np.random.randn(X.shape[1])\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        self.times = [0]\n",
    "        time_now = 0\n",
    "        \n",
    "        self.accuracy = []\n",
    "        self.accuracy.append(self.w[0])\n",
    "        \n",
    "        while np.linalg.norm(grad, ord = 2) > eps:\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            z = sigmoid(X @ self.w[0])\n",
    "            \n",
    "            grad = self._calc_grad(X, y, z, lambd)\n",
    "            \n",
    "            if len(losses) % 100 == 0:\n",
    "                self.accuracy.append(self.w[0] - lr * grad + beta * (self.w[0] - self.w[1]))\n",
    "                \n",
    "            crutch = self.w[0]\n",
    "            \n",
    "            self.w[0] -= lr * grad + beta * (self.w[0] - self.w[1])\n",
    "            \n",
    "            self.w[1] = crutch\n",
    "            \n",
    "            losses.append(self._loss(y, z))\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            time_now += end_time - start_time\n",
    "            \n",
    "            if len(losses) % 100 == 0:\n",
    "                self.times.append(time_now)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return sigmoid(X @ self.w[0])\n",
    "    \n",
    "    def get_times(self):\n",
    "        return self.times\n",
    "    \n",
    "    def get_accuracy(self, X, y):\n",
    "        accuracy = []\n",
    "        for i in range(len(self.accuracy)):\n",
    "            if i != 0 and accuracy_score(np.round(sigmoid(X @ self.accuracy[i])), y) == 1.0:\n",
    "                break\n",
    "            accuracy.append(accuracy_score(np.round(sigmoid(X @ self.accuracy[i])), y))\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcaa984",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Смотрим точность по метрике accuracy_score. Это логично, тк мы решаем задачу классификации.\n",
    "regressor = MyHeavyBall()\n",
    "regressor.fit(X_train, y_train, 1/L, -0.77777778, eps = 10**(-2))\n",
    "accuracy1 = regressor.get_accuracy(X_test, y_test)\n",
    "print(r'Точность по метрике accuracy_score при epsilon = 0.01: ', accuracy_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe201d2",
   "metadata": {},
   "source": [
    "__(б)__ Решите задачу логистической регрессии с помощью метода тяжелого шарика (не забудьте разделить выборку на две части: обучающую и тестовую). Зафиксируйте шаг $\\frac{1}{L}$ и перебирайте разные значения моментума от -1 до 1. Постройте график сходимости метода от числа итераций (критерий сходимости подберите самостоятельно) для различных значений моментума. Всегда ли сходимость является монотонной?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77fd67",
   "metadata": {},
   "source": [
    "Используем стандартный критерий $\\| \\nabla f\\| < 10^{-2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40582dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearch по значениям моментума\n",
    "Betas = np.linspace(-1, 1, 9)\n",
    "accuracy = []\n",
    "x = []\n",
    "for i in range(len(Betas)):\n",
    "    regressor = MyHeavyBall()\n",
    "    regressor.fit(X_train, y_train, 1/L, Betas[i])\n",
    "    accuracy.append(regressor.get_accuracy(X_test, y_test))\n",
    "    x.append(100 * np.array(range(len(accuracy[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize = (15, 15))\n",
    "fig.suptitle(r'Графики сходимости для различных значений $\\beta$', y = 0.93, fontsize = 30)\n",
    "k = 0 \n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axs[i][j].plot(x[k][1:], accuracy[k][1:], label = r'$\\beta = $' + str(np.round(Betas[k], 2)))\n",
    "        axs[i][j].set_ylabel(\"accuracy_score\")\n",
    "        axs[i][j].set_xlabel('Номер итерации')\n",
    "        axs[i][j].grid(alpha = 0.2)\n",
    "        axs[i][j].legend(loc = 'center right')\n",
    "        k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fb31fc",
   "metadata": {},
   "source": [
    "__(в)__ Для лучшего значения моментума постройте график зависимости точности модели на тестовой выборке от времени работы метода. Добавьте на этот же график сходимость градиентного спуска с шагом $\\frac{1}{L}$. Сделайте вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9688ca1",
   "metadata": {},
   "source": [
    "Как видно из графиков, лучшее значение моментума получилось $\\beta = 0.75$, посмотрим на скорость:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab5eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Linear_Regression:\n",
    "    def __init__(self, fit_intercept = True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "    \n",
    "    def fit(self, X, y, gamma, eps = 10**(-3)):\n",
    "        n, k = X.shape\n",
    "        \n",
    "        self.w = np.random.randn(k + 1 if self.fit_intercept else k)\n",
    "        \n",
    "        X = np.hstack((X, np.ones((n, 1)))) if self.fit_intercept else X\n",
    "        \n",
    "        self.times = [0]\n",
    "        time_now = 0\n",
    "        \n",
    "        self.accuracy = []\n",
    "        self.accuracy.append(self.w)\n",
    "        self.losses = []\n",
    "        grad = np.random.randn(k + 1 if self.fit_intercept else k)\n",
    "        \n",
    "        self.grads = [np.linalg.norm(grad, ord = 2)]\n",
    "        while np.linalg.norm(grad, ord = 2) > eps:\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            y_pred = self.predict(X)\n",
    "            self.losses.append(MSE(y_pred, y))\n",
    "\n",
    "            grad = self._calc_gradient(X, y, y_pred)\n",
    "            self.grads.append(np.linalg.norm(grad, ord = 2))\n",
    "            assert grad.shape == self.w.shape, f\"gradient shape {grad.shape} is not equal weight shape {self.w.shape}\"\n",
    "            \n",
    "            self.w -= gamma * grad\n",
    "            \n",
    "            if len(self.grads) % 100 == 0: \n",
    "                #print(f'Занчение нормы градиента на {len(self.grads)} итерациии - ', np.linalg.norm(grad, ord = 2))\n",
    "                self.accuracy.append(self.w - gamma * grad)\n",
    "                \n",
    "            end_time = time.time()\n",
    "            \n",
    "            time_now += end_time - start_time\n",
    "            \n",
    "            if len(self.grads) % 100 == 0:\n",
    "                self.times.append(time_now)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        n, _ = X.shape\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((X, np.ones((n, 1))))\n",
    "        \n",
    "        assert X.shape[1] == self.w.shape[0], f\"жопа в предикте\"\n",
    "        \n",
    "        y_pred = X @ self.w\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def _calc_gradient(self, X, y, y_pred):\n",
    "        \n",
    "        n, _ = X.shape\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((X, np.ones((n, 1))))\n",
    "        \n",
    "        grad = (2 / n) * (X.T @ (y_pred - y))\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def get_losses(self):\n",
    "        \n",
    "        return self.losses\n",
    "    \n",
    "    def get_accuracy(self, X, y):\n",
    "        accuracy = []\n",
    "        for i in range(len(self.accuracy)):\n",
    "            if i != 0 and accuracy_score(np.round(X @ self.accuracy[i]) , y) == 1.0:\n",
    "                break\n",
    "            accuracy.append(accuracy_score(np.round(X @ self.accuracy[i]) , y))\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def get_grads(self):\n",
    "        \n",
    "        return self.grads\n",
    "    \n",
    "    def get_times(self):\n",
    "        return self.times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9647ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lin_reg = My_Linear_Regression(fit_intercept = False)\n",
    "Heavy = MyHeavyBall()\n",
    "Lin_reg.fit(X_train, y_train, 0.095725, eps = 10**(-3))\n",
    "Heavy.fit(X_train, y_train, 1/L, 0.75, eps = 10**(-3))\n",
    "fig = plt.figure(figsize = (15, 10))\n",
    "\n",
    "plt.plot(Lin_reg.get_times()[:len(Lin_reg.get_accuracy(X_test, y_test))], Lin_reg.get_accuracy(X_test, y_test), \n",
    "         color = 'red', label = 'Обычная линейная регрессия')\n",
    "\n",
    "plt.plot(Heavy.get_times()[:len(Heavy.get_accuracy(X_test, y_test)[:-1])], Heavy.get_accuracy(X_test, y_test)[:-1],\n",
    "         color = 'black',  label = 'Тяжелый шарик')\n",
    "\n",
    "plt.title(\"Зависимость accuracy от времени работы\")\n",
    "plt.xlabel(\"Время, сек\")\n",
    "plt.ylabel(\"Точность по метрике accuracy_score\")\n",
    "\n",
    "plt.legend(loc = 'center right')\n",
    "plt.grid(alpha = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258b7b5",
   "metadata": {},
   "source": [
    "Как видно, получилось, что тяжелый шарик работает дольше, чем обычная линейная регрессия, но, как было показано ранее, может показать лушее значение accuracy_score для заданного значения критерия."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f76c5",
   "metadata": {},
   "source": [
    "__(г)__ Если в пунктах (б) и (в) столкнулись с проблемами, попробуйте $\\ell_2$-регуляризовать задачу или рассмотреть ее на ограниченном множестве."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a802e13",
   "metadata": {},
   "source": [
    "С проблемами не столкнулся."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7712495",
   "metadata": {},
   "source": [
    "__(д)__ Реализуйте ускоренный метод Нестерова (в форме Нестерова, а не который доказывали на семинаре). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nesterov(MyLogisticRegression):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs) # передаем именные параметры родительскому конструктору\n",
    "        self.w = None\n",
    "    \n",
    "    def fit(self, X, y, lr, beta, const = 0, lambd = False, eps = 10**(-2)):\n",
    "        \n",
    "        if self.w is None: self.w = np.random.randn(2, X.shape[1])\n",
    "        \n",
    "        grad = np.random.randn(X.shape[1])\n",
    "        \n",
    "        losses = []\n",
    "        losses.append(np.linalg.norm(grad, ord = 2))\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        self.times = [0]\n",
    "        time_now = 0\n",
    "        \n",
    "        self.accuracy = []\n",
    "        self.accuracy.append(self.w[0])\n",
    "        \n",
    "        while np.linalg.norm(grad, ord = 2) > eps:\n",
    "            if const == 0: \n",
    "                start_time = time.time()\n",
    "\n",
    "                crutch = self.w[0] + beta * (self.w[0] - self.w[1])\n",
    "\n",
    "                self.w[1] = self.w[0]\n",
    "\n",
    "                z = sigmoid(X @ crutch)\n",
    "\n",
    "                grad = self._calc_grad(X, y, z, lambd)\n",
    "\n",
    "                self.w[0] = crutch - lr * grad\n",
    "\n",
    "                if len(losses) % 100 == 0:\n",
    "                    self.accuracy.append(crutch - lr * grad)\n",
    "\n",
    "                losses.append(np.linalg.norm(grad, ord = 2))\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                time_now += end_time - start_time\n",
    "\n",
    "                if len(losses) % 100 == 0:\n",
    "                    self.times.append(time_now)\n",
    "            else:\n",
    "                start_time = time.time()\n",
    "\n",
    "                crutch = self.w[0] + (i / (i + const)) * (self.w[0] - self.w[1])\n",
    "\n",
    "                self.w[1] = self.w[0]\n",
    "\n",
    "                z = sigmoid(X @ crutch)\n",
    "\n",
    "                grad = self._calc_grad(X, y, z, lambd)\n",
    "\n",
    "                self.w[0] = crutch - lr * grad\n",
    "\n",
    "                if len(losses) % 10 == 0:\n",
    "                    self.accuracy.append(crutch - lr * grad)\n",
    "\n",
    "                losses.append(np.linalg.norm(grad, ord = 2))\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                time_now += end_time - start_time\n",
    "\n",
    "                if len(losses) % 10 == 0:\n",
    "                    self.times.append(time_now)\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "        return losses\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return sigmoid(X @ self.w[0])\n",
    "    \n",
    "    def get_times(self):\n",
    "        return self.times\n",
    "    \n",
    "    def get_accuracy(self, X, y):\n",
    "        accuracy = []\n",
    "        for i in range(len(self.accuracy)):\n",
    "            if i != 0 and accuracy_score(np.round(sigmoid(X @ self.accuracy[i])), y) == 1.0:\n",
    "                break\n",
    "            accuracy.append(accuracy_score(np.round(sigmoid(X @ self.accuracy[i])), y))\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f78fc",
   "metadata": {},
   "source": [
    "__(е)__ Решите задачу логистической регресии с помощью метода Нестерова (не забудьте разделить выборку на две части: обучающую и тестовую). Зафиксируйте шаг $\\frac{1}{L}$ и перебирайте разные значения моментума от -1 до 1. Проверьте также значения моментума равные $\\frac{k}{k+3}$, $\\frac{k}{k+2}$, $\\frac{k}{k+1}$ ($k$ - номер итерации), а если решаете сильно выпуклую задачу, то и $\\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}$. Постройте график сходимости метода от числа итераций (критерий сходимости подберите самостоятельно) для различных значений моментума. Всегда ли сходимость является монотонной?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ищем моменумы\n",
    "Betas = [-0.5, 0, 0.5, 0, 0, 0]\n",
    "losses = []\n",
    "x = []\n",
    "for i in range(len(Betas)):\n",
    "    if i < 3:\n",
    "        regressor = Nesterov()\n",
    "        losses.append(regressor.fit(X_train, y_train, 1/L, Betas[i], eps = 10**(-2)))\n",
    "        x.append(np.array(range(len(losses[i]))))\n",
    "    else: \n",
    "        regressor = Nesterov()\n",
    "        losses.append(regressor.fit(X_train, y_train, 1/L, Betas[i], const = i - 2, eps = 10**(-2)))\n",
    "        x.append(np.array(range(len(losses[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 7))\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(x[i][1:], losses[i][1:], label = r'$\\beta = $' + str(Betas[i]))\n",
    "plt.plot(x[3][1:], losses[3][1:], label = r'$\\beta = \\dfrac{k}{k + 1}$')\n",
    "plt.plot(x[4][1:], losses[4][1:], label = r'$\\beta = \\dfrac{k}{k + 2}$')\n",
    "plt.plot(x[5][1:], losses[5][1:], label = r'$\\beta = \\dfrac{k}{k + 3}$')\n",
    "\n",
    "plt.title(\"Графики сходимости в логарифмическом масштабе\")\n",
    "plt.xlabel(\"Номер итерации\")\n",
    "plt.ylabel(\"Значение критерия\")\n",
    "\n",
    "plt.text(1300, 0.5, r'Критерий $||\\nabla f\\:|| < 10^{-2}$', fontsize = 20)\n",
    "plt.yscale('log')\n",
    "plt.legend(loc = 'center right')\n",
    "plt.grid(alpha = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed787f",
   "metadata": {},
   "source": [
    "Как видно по последним трем моментумам(особенно отчетливо для красной линии), сходимость действительно может быть немонотонная."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74034f6",
   "metadata": {},
   "source": [
    "__(ж)__ Для лучшего значения моментума постройте график зависимости точности модели на тестовой выборке от времени работы метода. Добавьте этот график к графикам для тяжелого шарика и градиентного спуска из пункта (г). Сделайте итоговый вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ac3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Nest.fit(X_train, y_train, 1/L, 0, const = 1, eps = 10**(-2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bcfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lin_reg = My_Linear_Regression(fit_intercept = False)\n",
    "Heavy = MyHeavyBall()\n",
    "Nest = Nesterov()\n",
    "\n",
    "Lin_reg.fit(X_train, y_train, 0.095725, eps = 10**(-3))\n",
    "Heavy.fit(X_train, y_train, 1/L, 0.75, eps = 10**(-3))\n",
    "Nest.fit(X_train, y_train, 1/L, 0, const = 1, eps = 10**(-3))\n",
    "\n",
    "fig = plt.figure(figsize = (15, 10))\n",
    "\n",
    "plt.plot(Lin_reg.get_times()[:len(Lin_reg.get_accuracy(X_test, y_test))], Lin_reg.get_accuracy(X_test, y_test), \n",
    "         color = 'red', label = 'Обычная линейная регрессия')\n",
    "\n",
    "plt.plot(Heavy.get_times()[:len(Heavy.get_accuracy(X_test, y_test)[:-1])], Heavy.get_accuracy(X_test, y_test)[:-1],\n",
    "         color = 'black',  label = 'Тяжелый шарик')\n",
    "\n",
    "plt.plot(Nest.get_times()[:len(Nest.get_accuracy(X_test, y_test)[:-1])], Nest.get_accuracy(X_test, y_test)[:-1],\n",
    "         color = 'grey',  label = 'Нестеров')\n",
    "\n",
    "plt.title(\"Зависимость accuracy от времени работы\")\n",
    "plt.xlabel(\"Время, сек\")\n",
    "plt.ylabel(\"Точность по метрике accuracy_score\")\n",
    "\n",
    "plt.text(6, 0.9, r'Критерий $||\\nabla f\\:|| < 10^{-3}$', fontsize = 20)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.grid(alpha = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e46b4d",
   "metadata": {},
   "source": [
    "Вывод: логистическая регрессия решается градиентным спуском хорошо, метод тяжелого шарика улучшает точность решения, а метод нестерова и точность и скорость сходимости."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56317873",
   "metadata": {},
   "source": [
    "__Бонусные пункт__\n",
    "\n",
    "__(з)__ Сделаем подбор константы $L$ адаптивным. Как упоминалось на семинаре, можно измерять локальную $L$, используя:\n",
    "$$\n",
    "f(y) \\leq f(x^k) + \\langle \\nabla f(x^k), y - x^k \\rangle + \\frac{L}{2}\\|x^k - y\\|_2^2\n",
    "$$\n",
    "В частности, может подойти процедура:\n",
    "\n",
    "```python\n",
    "def backtracking_L(f, grad, x, h, L0, rho):\n",
    "    L = L0\n",
    "    fx = f(x)\n",
    "    gradx = grad(x)\n",
    "    while True:\n",
    "        y = x - 1 / L * h\n",
    "        if f(y) <= fx - 1 / L gradx.dot(h) + 1 / (2 * L) h.dot(h):\n",
    "            break\n",
    "        else:\n",
    "            L = L * rho\n",
    "    return L\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87c7ad",
   "metadata": {},
   "source": [
    "Каким стоит взять __h__? __rho__ должно быть больше или меньше 1? __L0__ надо брать заведомо большим или маленьким?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd9efb5",
   "metadata": {},
   "source": [
    "__(и)__ Поэксперементируйте с этой процедурой, встроенной в подбор $L$ для шага градиентного спуска. В качестве задачи продолжайте рассматривать логистическую регрессию из Задачи 1. Аналогично встройте процедуру подбора $L$ в метод тяжелого шарика и ускоренный метод Нестерова. Постройте график сходимости метода от числа итераций (критерий сходимости подберите самостоятельно). Отобразите на этом графике градиентный спуск, тяжелый шарик и метод Нестерова с адаптивным шагом и шагом $\\frac{1}{L}$ (всего 6 линий на графике). Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d242c4",
   "metadata": {},
   "source": [
    "__(к)__ Постройте аналогичный пункту (и) график точности модели от времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e880742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0ffdd",
   "metadata": {},
   "source": [
    "__(л)__ В [работе](https://arxiv.org/pdf/1204.3982.pdf) представлена техника рестартов для подавления немонотонной сходимости Алгоритма 2 (метод Нестерова). Попробуйте повторить эксперименты авторов на $\\ell_2$-регуляризованной квадратичной или логистической регресии. Возьмите параметр регуляризации $\\lambda = L / 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ответ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
